#!/usr/bin/env python3
"""
Meta Evaluation Analysis Script
Analyzes results generated by metajudge_infer.py, computing Recall/Precision/F1 metrics

Core Features:
1. Read inference result files
2. Calculate Recall, Precision, F1, Average Precision for each model
3. Output results sorted by specified metric
"""

import argparse
import json
import os
import re
from typing import List, Dict, Optional, Tuple
from collections import defaultdict
import numpy as np


# ============================================================================
# Result Extraction Module
# ============================================================================

def extract_result_scores(response: str) -> dict:
    """
    Extract scoring information between RESULT_START and RESULT_END tags
    
    Args:
        response: Response text containing scoring results
        
    Returns:
        dict: Dictionary containing extraction results:
        {
            'scores': [(R_num, S_num, score), ...],  # List of tuples
            'is_ordered': bool,  # Whether Rx are in sequence
            'errors': []  # List of error messages
        }
    """
    result = {
        'scores': [],
        'is_ordered': True,
        'errors': []
    }
    
    # Extract content between RESULT_START and RESULT_END
    result_pattern = r'<RESULT_START>(.*?)<RESULT_END>'
    match = re.search(result_pattern, response, re.DOTALL)
    
    if not match:
        result['errors'].append("RESULT_START and RESULT_END tags not found")
        return result
    
    result_content = match.group(1)
    
    # Extract score lines in format: - Rx@Sy: score
    score_pattern = r'- R(\d+)@S(\d+): (\d+(?:\.\d+)?)'
    matches = re.findall(score_pattern, result_content)
    
    if not matches:
        result['errors'].append("No score lines matching the expected format found")
        return result
    
    # Parse matched results
    r_numbers = []
    for m in matches:
        try:
            r_num = int(m[0])
            s_num = int(m[1])
            score = float(m[2])
            
            # Check score range
            if not (0 <= score <= 1):
                result['errors'].append(f"Score {score} for R{r_num}@S{s_num} is not in range 0-1")
            
            result['scores'].append((r_num, s_num, score))
            r_numbers.append(r_num)
            
        except ValueError as e:
            result['errors'].append(f"Parse error: {m} - {str(e)}")
    
    # Check if Rx are in sequence
    if len(r_numbers) > 1:
        for i in range(1, len(r_numbers)):
            if r_numbers[i] != r_numbers[i-1] + 1:
                result['is_ordered'] = False
                break
    
    return result


# ============================================================================
# Metrics Calculation Module
# ============================================================================

def calculate_metrics(scores: List[Tuple[int, int, float]], 
                     num_reference: int, 
                     num_original: int) -> Optional[Dict]:
    """
    Calculate precision, recall, F1 score, and average precision
    
    Args:
        scores: List of tuples [(r_num, s_num, score), ...]
            - r_num: Reference item number
            - s_num: Original item number (0 means no match)
            - score: Matching score (0-1)
        num_reference: Number of reference items (human checklist)
        num_original: Number of original items (model checklist)
    
    Returns:
        dict: Dictionary containing precision, recall, f1, average_precision
    """
    if not scores:
        return None
    if len(scores) != num_reference:
        return None
    
    # Filter out items where s_num is 0 (no match)
    valid_scores = [(r_num, s_num, score) for r_num, s_num, score in scores 
                   if s_num != 0 and r_num <= num_reference and s_num <= num_original]
    
    # Calculate recall: for each reference item, take the highest matching score
    if not valid_scores:
        recall = 0.0
    else:
        # Group by r_num, take max score for each group
        r_num_to_max_score = {}
        for r_num, s_num, score in valid_scores:
            if r_num not in r_num_to_max_score:
                r_num_to_max_score[r_num] = score
            else:
                r_num_to_max_score[r_num] = max(r_num_to_max_score[r_num], score)
        
        # Recall = sum of max matching scores / number of reference items
        recall_numerator = sum(r_num_to_max_score.values())
        recall = recall_numerator / num_reference if num_reference > 0 else 0.0
    
    # Calculate precision: for each original item, take the highest matching score
    if not valid_scores:
        precision = 0.0
    else:
        # Group by s_num, take max score for each group
        s_num_to_max_score = {}
        for r_num, s_num, score in valid_scores:
            if s_num not in s_num_to_max_score:
                s_num_to_max_score[s_num] = score
            else:
                s_num_to_max_score[s_num] = max(s_num_to_max_score[s_num], score)
        
        # Precision = sum of max matching scores / number of original items
        precision_numerator = sum(s_num_to_max_score.values())
        precision = precision_numerator / num_original if num_original > 0 else 0.0
    
    # Calculate average precision
    if not valid_scores:
        average_precision = 0.0
    else:
        s_num_to_max_score = {}
        for r_num, s_num, score in valid_scores:
            if s_num not in s_num_to_max_score:
                s_num_to_max_score[s_num] = score
            else:
                s_num_to_max_score[s_num] = max(s_num_to_max_score[s_num], score)
        
        sorted_s_nums = sorted(s_num_to_max_score.keys())
        cumulative_precision = 0.0
        for i, s_num in enumerate(sorted_s_nums):
            current_precision = sum(s_num_to_max_score[s] for s in sorted_s_nums[:i+1]) / (i + 1)
            cumulative_precision += current_precision
        
        average_precision = cumulative_precision / num_reference
    
    # Calculate F1 score
    if precision + recall == 0:
        f1 = 0.0
    else:
        f1 = 2 * precision * recall / (precision + recall)
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'average_precision': average_precision
    }


# ============================================================================
# Data Loading Module
# ============================================================================

def load_jsonl(filepath: str) -> List[Dict]:
    """Load JSONL file"""
    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    item = json.loads(line)
                    data.append(item)
                except json.JSONDecodeError:
                    pass
    return data


def analyze_single_file(filepath: str, model_be_evaluated: str, 
                        max_checklist_len: int = 10) -> Dict:
    """
    Analyze a single inference result file
    
    Args:
        filepath: Path to inference result file
        model_be_evaluated: Name of the model being evaluated
        max_checklist_len: Maximum checklist length limit
    
    Returns:
        dict: Dictionary containing analysis results
    """
    data = load_jsonl(filepath)
    
    precision_list, recall_list, f1_list, ap_list = [], [], [], []
    valid_count = 0
    error_count = 0
    
    model_checklist_key = f"{model_be_evaluated}-checklist"
    
    for item in data:
        if 'samples' not in item or len(item['samples']) == 0:
            error_count += 1
            continue
        
        eval_text = item['samples'][0]['generated_text']
        ref_item = item.get('metadata', {})
        
        # Extract scores
        extract_results = extract_result_scores(eval_text)
        if extract_results['errors']:
            error_count += 1
            continue
        
        # Find checklist field (case-insensitive)
        found_key = None
        for key in ref_item.keys():
            if key.lower() == model_checklist_key.lower():
                found_key = key
                break
        
        if not found_key or 'human-checklist' not in ref_item:
            error_count += 1
            continue
        
        human_checklist = ref_item['human-checklist']
        model_checklist = ref_item[found_key]
        
        # Filter checklists that are too long
        if len(model_checklist) > max_checklist_len:
            continue
        if len(model_checklist) == 0:
            continue
        
        # Calculate metrics
        results = calculate_metrics(
            extract_results['scores'],
            len(human_checklist),
            min(len(model_checklist), 5)  # Limit original checklist length
        )
        
        if not results:
            error_count += 1
            continue
        
        precision_list.append(results['precision'])
        recall_list.append(results['recall'])
        f1_list.append(results['f1'])
        ap_list.append(results['average_precision'])
        valid_count += 1
    
    return {
        'model': model_be_evaluated,
        'total_samples': len(data),
        'valid_samples': valid_count,
        'error_samples': error_count,
        'precision': np.mean(precision_list) if precision_list else 0.0,
        'recall': np.mean(recall_list) if recall_list else 0.0,
        'f1': np.mean(f1_list) if f1_list else 0.0,
        'average_precision': np.mean(ap_list) if ap_list else 0.0,
        'precision_std': np.std(precision_list) if precision_list else 0.0,
        'recall_std': np.std(recall_list) if recall_list else 0.0,
    }


def analyze_directory(directory: str, eval_model: str = None, 
                     max_checklist_len: int = 10) -> List[Dict]:
    """
    Analyze all inference result files in a directory
    
    Args:
        directory: Directory containing inference results
        eval_model: Evaluator model name (for filtering files)
        max_checklist_len: Maximum checklist length limit
    
    Returns:
        list: List of analysis results for all models
    """
    results = []
    
    for filename in os.listdir(directory):
        if not filename.endswith('.jsonl'):
            continue
        
        filepath = os.path.join(directory, filename)
        name_without_ext = filename.replace('.jsonl', '')
        
        # Try to get model name from file content
        model_be_evaluated = None
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    first_line = f.readline().strip()
                    if first_line:
                        data = json.loads(first_line)
                        metadata = data.get('metadata', {})
                        # Find checklist field to infer model name
                        for key in metadata.keys():
                            if key.endswith('-checklist') and key != 'human-checklist':
                                model_be_evaluated = key.replace('-checklist', '')
                                break
            except Exception as e:
                print(f"Failed to read file {filename}: {e}")
                continue
        
        # If not found from file, use filename as model name
        if not model_be_evaluated:
            model_be_evaluated = name_without_ext
        
        # Filter by eval_model if specified
        if eval_model and eval_model not in filename:
            continue
        
        # Skip input files (not inference results)
        if 'input' in filename.lower() and 'output' not in filename.lower():
            continue
        
        print(f"Analyzing file: {filename} (model: {model_be_evaluated})")
        
        result = analyze_single_file(filepath, model_be_evaluated, max_checklist_len)
        if result['valid_samples'] > 0:
            results.append(result)
    
    return results


def print_results_table(results: List[Dict], sort_by: str = 'recall'):
    """
    Print results table
    
    Args:
        results: List of analysis results
        sort_by: Sort field ('recall', 'precision', 'f1', 'average_precision')
    """
    # Sort
    sorted_results = sorted(results, key=lambda x: x[sort_by], reverse=True)
    
    # Print header
    print(f"\n{'='*100}")
    print(f"Results Sorted by {sort_by.upper()}")
    print(f"{'='*100}")
    print(f"{'Model':<45} {'Precision':<12} {'Recall':<12} {'F1':<12} {'AP':<12} {'Valid':<8}")
    print(f"{'-'*100}")
    
    # Print data
    for r in sorted_results:
        print(f"{r['model']:<45} {r['precision']:<12.4f} {r['recall']:<12.4f} "
              f"{r['f1']:<12.4f} {r['average_precision']:<12.4f} {r['valid_samples']:<8}")
    
    print(f"{'='*100}\n")


def save_results_json(results: List[Dict], output_file: str):
    """Save results to JSON file"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"Results saved to: {output_file}")


# ============================================================================
# Main Function
# ============================================================================

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(description='Meta Evaluation Analysis Script')
    
    parser.add_argument('--input-file', type=str, default=None,
                       help='Path to single inference result file')
    parser.add_argument('--input-dir', type=str, default=None,
                       help='Directory containing multiple inference result files')
    parser.add_argument('--model-be-evaluated', type=str, default=None,
                       help='Model name being evaluated (required when analyzing single file)')
    parser.add_argument('--eval-model', type=str, default=None,
                       help='Evaluator model name (for filtering files in directory)')
    parser.add_argument('--max-checklist-len', type=int, default=10,
                       help='Maximum checklist length limit (default: 10)')
    parser.add_argument('--sort-by', type=str, default='recall',
                       choices=['recall', 'precision', 'f1', 'average_precision'],
                       help='Sort field (default: recall)')
    parser.add_argument('--output-file', type=str, default=None,
                       help='Output JSON file path')
    
    return parser.parse_args()


def main():
    """Main function"""
    args = parse_arguments()
    
    results = []
    
    if args.input_file:
        # Analyze single file
        if not args.model_be_evaluated:
            # Try to parse from filename
            filename = os.path.basename(args.input_file)
            args.model_be_evaluated = filename.split('-')[0]
        
        print(f"Analyzing file: {args.input_file}")
        result = analyze_single_file(
            args.input_file, 
            args.model_be_evaluated,
            args.max_checklist_len
        )
        results.append(result)
        
    elif args.input_dir:
        # Analyze directory
        print(f"Analyzing directory: {args.input_dir}")
        results = analyze_directory(
            args.input_dir,
            args.eval_model,
            args.max_checklist_len
        )
    else:
        print("Error: Please specify --input-file or --input-dir")
        return
    
    if not results:
        print("No valid analysis results found")
        return
    
    # Print results
    print_results_table(results, args.sort_by)
    
    # Save results
    if args.output_file:
        save_results_json(results, args.output_file)


if __name__ == "__main__":
    main()
